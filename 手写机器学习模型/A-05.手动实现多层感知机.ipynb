{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动实现MLP\n",
    "\n",
    "multi-layer preception\n",
    "\n",
    "**关键点**\n",
    "\n",
    "1. 反向传播公式推导\n",
    "2. 常见激活函数及其梯度\n",
    "3. 算法整体流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "from tensorflow.keras import datasets\n",
    "mnist = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 定义MLP类\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "class MLP:\n",
    "    def __init__(self,sizes):\n",
    "        shape ,self.activations = sizes\n",
    "        self.w = [np.random.randn(c1,c2) for c1,c2 in zip(shape[:-1],shape[1:])]\n",
    "        self.b = [np.zeros(c) for c in shape[1:]]\n",
    "        self.layers = len(shape)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for w,b,act in zip(self.w,self.b,self.activations):\n",
    "            u = h @ w + b\n",
    "            h = eval(act)(u)\n",
    "        return h\n",
    "        \n",
    "    def backprop(self,data):\n",
    "        '''\n",
    "        计算参数的梯度  SGD\n",
    "        首先进行前向传播，记录每一层的输出（未通过激活函数的和通过激活函数的）\n",
    "        记录 zip后一层的梯度\n",
    "        \n",
    "        '''\n",
    "        grads_w = [np.zeros(w.shape) for w in self.w]\n",
    "        grads_b = [np.zeros(b.shape) for b in self.b]\n",
    "        \n",
    "        x,y = data\n",
    "        # 记录每层的输出\n",
    "        u_val = []\n",
    "        h_val = [x]\n",
    "        h = x\n",
    "        for w,b,act in zip(self.w,self.b,self.activations):\n",
    "            u = h @ w + b\n",
    "            h = eval(act)(u)\n",
    "            u_val.append(u)\n",
    "            h_val.append(h)\n",
    "        # 计算交叉熵\n",
    "        loss = crossentropy(y,h)\n",
    "        # 计算交叉熵的导数\n",
    "        d_loss_ce = d_corssentropy(y,h)\n",
    "        # 计算各层激活函数的导数\n",
    "        d_fun = []\n",
    "        for u,act in zip(u_val,self.activations):  # \n",
    "            d_act = 'd_' + act\n",
    "            d_fun.append(eval(d_act)(u))\n",
    "\n",
    "        nab = d_loss_ce @ d_fun[-1]\n",
    "        grads_w[-1] = h_val[-2][:,np.newaxis] @ nab[np.newaxis,:]\n",
    "        grads_b[-1] = nab\n",
    "        for i,w in zip(range(2,self.layers),reversed(self.w[1:])):\n",
    "            i = -i\n",
    "            \n",
    "            nab = (nab @ w.T) @ d_fun[i]\n",
    "            grads_w[i] = h_val[i-1][:,np.newaxis] @ nab[np.newaxis,:]\n",
    "            grads_b[i] = nab\n",
    "                \n",
    "        return grads_w,grads_b,loss\n",
    "        \n",
    "        \n",
    "    def train(self,train_data,test_data,lr = 0.1,epoch = 10,batch_size = 100):\n",
    "        '''\n",
    "        :train_data:  [[x] ,[y]]\n",
    "        :test_data:   [[x] ,[y]]\n",
    "        '''\n",
    "        # SGD\n",
    "        x,y = train_data\n",
    "        data = list(zip(x,y))\n",
    "        random.shuffle(data)\n",
    "\n",
    "        data_slice = [data[k:k+batch_size] for k in range(0,len(data),batch_size)]\n",
    "        for j in range(epoch):\n",
    "            for i in range(len(data)//batch_size):\n",
    "                # get_batch\n",
    "                batch_data = data_slice[i]\n",
    "                # BP\n",
    "                acc = [\n",
    "                    [np.zeros(w.shape) for w in self.w],\n",
    "                    [np.zeros(b.shape) for b in self.b]]\n",
    "                \n",
    "                tot_loss = 0\n",
    "                for i_data in batch_data:\n",
    "                    grads_w,grads_b,loss = self.backprop(i_data)\n",
    "                    acc = [[a+g for a,g in zip(acc[0],grads_w)],[a+g for a,g in zip(acc[1],grads_b)]]\n",
    "                    tot_loss += loss\n",
    "                if i%(batch_size//5)==0:\n",
    "                    print('Current loss is {0}'.format(tot_loss))\n",
    "                    x_test,y_test = test_data\n",
    "                    predict = self.predict(x_test)\n",
    "                    accuracy = np.sum(np.where(predict==y_test,1,0)) / len(y_test)\n",
    "                    print('Step {0} : accuracy is {1} % '.format(j,accuracy*100))\n",
    "                \n",
    "                # update param\n",
    "                d_w,d_b = [dw/batch_size for dw in acc[0]],[db/batch_size for db in acc[1]]\n",
    "                self.w = [w - lr*dw for w,dw in zip(self.w,d_w)]\n",
    "                self.b = [b - lr*db for b,db in zip(self.b,d_b)]\n",
    "            # validation\n",
    "            # compute accuracy\n",
    "            \n",
    "        \n",
    "    def predict(self,x):            \n",
    "        return [np.argmax(self.forward(t)) for t in x]       \n",
    "        \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def d_sigmoid(x):        \n",
    "    return np.eye(x.shape[-1])*(sigmoid(x)*(1-sigmoid(x)))\n",
    "\n",
    "def softmax(x):\n",
    "    return np.array([np.exp(t) for t in x]) / np.sum(np.exp(t) for t in x)\n",
    "\n",
    "def d_softmax(x):\n",
    "    s = softmax(x)\n",
    "    mat = np.eye(x.shape[-1])\n",
    "    mat *= s\n",
    "    mask = s[:,np.newaxis] * s[np.newaxis,:]\n",
    "\n",
    "    return mat - mask\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x>0,x,0)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.eye(x.shape[-1])*np.where(x>0,1.,0.)\n",
    "\n",
    "\n",
    "def crossentropy(p,q):\n",
    "    '''\n",
    "    p 为真实的label（非one-hot）\n",
    "    q为预测概率\n",
    "    '''\n",
    "    return -np.log(q[p]) \n",
    "\n",
    "\n",
    "def d_corssentropy(p,q):\n",
    "    '''\n",
    "    p 为真实的label\n",
    "    q为预测概率\n",
    "    '''\n",
    "    one_hot = np.zeros(q.shape)\n",
    "    one_hot[p] = -1/q[p]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y),(x_test,y_test) = mnist\n",
    "# 归一化、faltten\n",
    "x = x.reshape([-1,28*28]) / 255.\n",
    "x_test = x_test.reshape([-1,28*28]) / 255.\n",
    "\n",
    "\n",
    "train_data = (x,y)\n",
    "test_data = (x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = MLP([[784,128,64,32,10],['sigmoid','sigmoid','sigmoid','softmax']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:112: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss is 1164.3824541627728\n",
      "Step 0 : accuracy is 9.68 % \n",
      "Current loss is 278.59111409368955\n",
      "Step 0 : accuracy is 12.94 % \n",
      "Current loss is 232.9602061534984\n",
      "Step 0 : accuracy is 17.14 % \n",
      "Current loss is 222.3801554470345\n",
      "Step 0 : accuracy is 21.65 % \n",
      "Current loss is 213.96921020310037\n",
      "Step 0 : accuracy is 25.230000000000004 % \n",
      "Current loss is 209.47310943948463\n",
      "Step 0 : accuracy is 28.939999999999998 % \n",
      "Current loss is 198.02824734364307\n",
      "Step 0 : accuracy is 32.23 % \n",
      "Current loss is 168.80944277846925\n",
      "Step 0 : accuracy is 35.75 % \n",
      "Current loss is 178.8659935916214\n",
      "Step 0 : accuracy is 38.48 % \n",
      "Current loss is 159.47079087770618\n",
      "Step 0 : accuracy is 40.93 % \n",
      "Current loss is 165.67221057662172\n",
      "Step 0 : accuracy is 43.04 % \n",
      "Current loss is 150.53254871784273\n",
      "Step 0 : accuracy is 45.32 % \n",
      "Current loss is 168.39612546593682\n",
      "Step 0 : accuracy is 47.02 % \n",
      "Current loss is 169.61453108032552\n",
      "Step 0 : accuracy is 49.0 % \n",
      "Current loss is 154.07976197159346\n",
      "Step 0 : accuracy is 50.49 % \n",
      "Current loss is 144.07478958828705\n",
      "Step 0 : accuracy is 51.349999999999994 % \n",
      "Current loss is 151.52648763996498\n",
      "Step 0 : accuracy is 52.949999999999996 % \n",
      "Current loss is 128.0052126574253\n",
      "Step 0 : accuracy is 54.120000000000005 % \n",
      "Current loss is 143.1652499464654\n",
      "Step 0 : accuracy is 55.26 % \n",
      "Current loss is 140.25188604089914\n",
      "Step 0 : accuracy is 56.510000000000005 % \n"
     ]
    }
   ],
   "source": [
    "ann.train(train_data,test_data,epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import datasets\n",
    "# moon = datasets.make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ann = MLP([[2,3,2],['sigmoid','sigmoid']])\n",
    "# # ann.train(moon,moon,epoch=20,batch_size = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
