{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树与随机森林\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- 参考：\n",
    "   - 统计学习方法的代码实现\n",
    "\n",
    "- 知识点\n",
    "  1. 基于特征对实例进行分类的树形结构\n",
    "  2. 试图找到最优的分割决策树（实际上是NP-C问题）\n",
    "  3. 核心是计算每个属性的分类效果统计量（信息增益、信息增益比、基尼系数）\n",
    "  4. 生成决策树时采用贪心思想递归划分子集\n",
    "  5. 为解决过拟合问题，进行剪枝；往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- 决策树家族\n",
    "   - ID3 样本集合$D$对特征$A$的信息增益（ID3）\n",
    "   \n",
    "   $$g(D, A)=H(D)-H(D|A)$$\n",
    "\n",
    "$$H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log _{2} \\frac{\\left|C_{k}\\right|}{|D|}$$\n",
    "\n",
    "$$H(D | A)=\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} H\\left(D_{i}\\right)$$\n",
    "   \n",
    "   - C4.5 样本集合$D$对特征$A$的信息增益比\n",
    "   \n",
    "   $$g_{R}(D, A)=\\frac{g(D, A)}{H(D)}$$\n",
    "   \n",
    "   - CART 样本集合$D$的基尼指数\n",
    "  \n",
    "  $$\\operatorname{Gini}(D)=1-\\sum_{k=1}^{K}\\left(\\frac{\\left|C_{k}\\right|}{|D|}\\right)^{2}$$\n",
    "\n",
    "特征$A$条件下集合$D$的基尼指数：\n",
    "\n",
    " $$\\operatorname{Gini}(D, A)=\\frac{\\left|D_{1}\\right|}{|D|} \\operatorname{Gini}\\left(D_{1}\\right)+\\frac{\\left|D_{2}\\right|}{|D|} \\operatorname{Gini}\\left(D_{2}\\right)$$\n",
    "  \n",
    "  - Random Forest\n",
    "  \n",
    "  \n",
    "- 编程\n",
    "   - 计算条件熵、信息熵、信息增益、信息增益比\n",
    "      - 熵计算公式   $H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log _{2} \\frac{\\left|C_{k}\\right|}{|D|}$\n",
    "      - 连续值如何计算熵\n",
    "         - 参考：  [决策树（decision tree）(三)——连续值处理](https://blog.csdn.net/u012328159/article/details/79396893)\n",
    "         - 将连续值观测量的观测结果排序，$a1,a2,a3,a4,...a_{n}$\n",
    "         - 取中值$m1,m2,m3,m4,..,m_{n-1} = (a1+a2)/2,(a2+a3)/2,...,(a_{n-1}+a_{n})/2$\n",
    "         - 按照每个中值$x>m_{i}$,$x<m_{i}$作为一个判断条件\n",
    "         - 计算每个中值对应的信息增益，选出其中具有最大信息增益的点作为该连续属性的判断条件\n",
    "     \n",
    "  - 使用sklearn的决策树模型\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息增益\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# 导入数据\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def import_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data,columns = iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    data = np.array(df)\n",
    "    return data[:,:-1] , data[:,-1] , iris.feature_names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self):\n",
    "        self.label = None\n",
    "        self.data = None\n",
    "        self.features = None\n",
    "        self.entropy = None\n",
    "        self.info_gain = None\n",
    "        pass\n",
    "    \n",
    "    def train(self,features,label,names):\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "        label = label[:,np.newaxis]\n",
    "        data = np.hstack((features,label))\n",
    "        col_name = names.copy()\n",
    "        \n",
    "        col_name.append('label')\n",
    "        self.data = pd.DataFrame(data,columns=col_name)\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_entropy(self,lis = None,attr = 'label'):\n",
    "        # 接收一个列表('label'值)，返回这个列表的label的熵\n",
    "        rec = False\n",
    "        if lis is None:\n",
    "            lis = list(self.data[attr])\n",
    "            rec = True\n",
    "            if self.entropy is not None:\n",
    "                return self.entropy\n",
    "        num = len(lis)\n",
    "        cnt = dict(Counter(lis))\n",
    "        \n",
    "#         print(cnt)\n",
    "        p = np.array(np.array(list(cnt.values()))/num)\n",
    "        ent = np.sum(p*np.log2(p))\n",
    "        ent = -ent\n",
    "        if rec == True:\n",
    "            self.entropy = ent\n",
    "        return ent\n",
    "        \n",
    "    def get_cond_entropy(self,attr,val,lab_attr = 'label'):\n",
    "        # H(D|A) = sum_each_a(p(a)*H(D|a))\n",
    "        arr = self.data[attr]\n",
    "        s1 = np.array(sorted(arr))\n",
    "        t1 =  len(np.where(s1 < val))\n",
    "        t = len(arr)\n",
    "        p_a = [t1/t,1-t1/t]\n",
    "        \n",
    "#         print(np.where(val > s1))\n",
    "        l1 = self.label[np.where(arr < val)]\n",
    "        l2 = self.label[np.where(arr >= val)]\n",
    "        H_cnd = [\n",
    "            self.get_entropy(lis = l1),self.get_entropy(lis = l2)\n",
    "        ]\n",
    "        return p_a[0]*H_cnd[0] + p_a[1]*H_cnd[1]\n",
    "    \n",
    "    def get_info_gain(self,attr,lab_attr = 'label'):\n",
    "        # 由于iris数据集属性的观测值是连续的，采用连续值熵计算公式\n",
    "        arr = self.data[attr]\n",
    "        s = sorted(arr,reverse=False)\n",
    "        s.append(0)\n",
    "        s1 = np.array(s[:-1])\n",
    "        s2 = np.array(s[1:])     \n",
    "        mid = (s1+s2)/2\n",
    "        mid = mid.tolist()[:-1]\n",
    "        mid_ig = {}\n",
    "        for index,item in enumerate(mid):\n",
    "            # 计算每个中值的信息增益\n",
    "            # IG(D|A) = H(D) - H(D|A)\n",
    "            mid_ig[index] = self.get_entropy() - self.get_cond_entropy(attr,item)\n",
    "        ind = max(mid_ig,key = mid_ig.get)\n",
    "        return mid[ind],mid_ig[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr --> (3.5, 0.8530242373675735)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# x,y,names = import_data()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n",
    "dt = DTree()\n",
    "# dt.train(X_train,y_train,names)\n",
    "xx = np.array([1,2,2,3,4,5,6,33])\n",
    "xx = xx[:,np.newaxis]\n",
    "yy = np.array([0,1,0,0,1,1,1,1])\n",
    "names = ['attr']\n",
    "dt.train(xx,yy,names)\n",
    "# print(y_train)\n",
    "\n",
    "for i in names:\n",
    "    print(i,'-->',dt.get_info_gain(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 调用sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "# import graphviz\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "ch",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "ch",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
